{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31e4fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/ml-deploy/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "# from gradio_demo.model.utils import output_conv_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device is: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad190b",
   "metadata": {},
   "source": [
    "Loading the dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a71f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Xenova/quickdraw-small\")\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0,), (1,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_ops(examples):\n",
    "    examples[\"image\"] = [preprocess(image) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "dataset.set_transform(preprocess_ops)\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"test\"],\n",
    "    dataset[\"valid\"],\n",
    ")\n",
    "train_dataset = train_dataset.shard(num_shards=3, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fe5e1",
   "metadata": {},
   "source": [
    "#### With data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67112611",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Xenova/quickdraw-small\")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Validation/Test: No random changes, just format\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    # Apply the \"noisy\" transforms to the list of images\n",
    "    examples[\"image\"] = [train_transforms(image) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "def preprocess_eval(examples):\n",
    "    # Apply the \"clean\" transforms\n",
    "    examples[\"image\"] = [eval_transforms(image) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset  = dataset[\"test\"]\n",
    "val_dataset   = dataset[\"valid\"]\n",
    "\n",
    "train_dataset = train_dataset.shard(num_shards=3, index=0)\n",
    "\n",
    "# 6. Apply the transforms to the specific splits\n",
    "train_dataset.set_transform(preprocess_train)\n",
    "test_dataset.set_transform(preprocess_eval)\n",
    "val_dataset.set_transform(preprocess_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e982d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of trainset: 1500000, testset: 250000\n"
     ]
    }
   ],
   "source": [
    "names = train_dataset.features[\"label\"].names\n",
    "n_classes = len(names)\n",
    "print(f\"size of trainset: {len(train_dataset)}, testset: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d013cd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x768cec4a6270>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"labels.json\", \"w\") as f:\n",
    "    json.dump(names, f)\n",
    "\n",
    "\n",
    "def id_to_class(idx):\n",
    "    return names[idx]\n",
    "\n",
    "\n",
    "# visualizing an example\n",
    "idx = torch.randint(0, 100, (1,))\n",
    "img, label = test_dataset[idx][\"image\"], test_dataset[idx][\"label\"]\n",
    "img = img[0].squeeze(dim=0)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694f9a2",
   "metadata": {},
   "source": [
    "Model architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743f9359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device is: {device}\")\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_filters,\n",
    "        hidden_dim,\n",
    "        n_layers,\n",
    "        n_classes,\n",
    "        input_shape=(1, img.shape[0], img.shape[1]),\n",
    "        dropout_rate=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(n_filters, 2 * n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(2 * n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_filters, 4 * n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(4 * n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(1, n_filters, conv_kernel_size)\n",
    "        # self.relu1 = nn.ReLU()\n",
    "        # self.maxpool1 = nn.MaxPool2d(maxpool_kernel_size)\n",
    "        # self.conv2 = nn.Conv2d(n_filters, 2*n_filters, conv_kernel_size)\n",
    "        # self.relu2 = nn.ReLU()\n",
    "        # self.maxpool2 = nn.MaxPool2d(maxpool_kernel_size)\n",
    "        # self.input_dim = output_conv_size(img.shape[0], img.shape[1],conv_kernel_size=conv_kernel_size,maxpool_kernel_size=maxpool_kernel_size,n_filters=n_filters)#960\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "        conv_out_size = self._get_flat_size(dummy_input)\n",
    "        # print(conv_out_size)\n",
    "        self.input_dim = conv_out_size # 1080  # 2940#1500 # 960\n",
    "        self.inp_layer = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.classifier = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.classifier.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=dropout_rate),\n",
    "                )\n",
    "            )\n",
    "        self.out_layer = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def _get_flat_size(self, x):\n",
    "        \"\"\"Helper class to get the flat size of the input after convolutions\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.inp_layer(torch.flatten(x, start_dim=1))\n",
    "        for layer in self.classifier:\n",
    "            x = layer(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb51eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params 579909\n"
     ]
    }
   ],
   "source": [
    "params = {\"n_filters\": 30, \"hidden_dim\": 256, \"n_layers\": 2, \"n_classes\": n_classes, \"dropout_rate\": 0.3}\n",
    "model = CNN(**params).to(device)\n",
    "n_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of params {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11b91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/ml-deploy/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 40 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 512\n",
    "n_epochs = 6\n",
    "num_workers = 40\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainLoader, testLoader = (\n",
    "    DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers),\n",
    "    DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ad7d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < (self.best_loss - self.min_delta):\n",
    "            # Loss improved! Save the model state and reset counter\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Loss didn't improve enough\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        \"\"\"Restores the model weights from the epoch with the best loss\"\"\"\n",
    "        if self.best_model_state:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "            print(f\"Restored best model with loss: {self.best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ee08a",
   "metadata": {},
   "source": [
    "Training loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e53e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, trainLoader, testLoader, criterion, optimizer, n_epochs, device):\n",
    "#     train_losses = []\n",
    "#     train_accs = []\n",
    "#     test_losses = []\n",
    "#     test_accs = []\n",
    "#     for epoch in range(1, n_epochs + 1):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         train_acc = 0\n",
    "#         for batch in tqdm(trainLoader):\n",
    "#             data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "#             out = model(data)\n",
    "#             preds = out.argmax(dim=1)\n",
    "#             loss = criterion(out, labels)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "#             train_acc += (preds == labels).sum().item()\n",
    "\n",
    "#         train_loss /= len(trainLoader)\n",
    "#         train_acc /= len(trainLoader.dataset)\n",
    "#         train_accs.append(train_acc)\n",
    "#         train_losses.append(train_loss)\n",
    "\n",
    "#         model.eval()\n",
    "#         test_loss = 0\n",
    "#         test_acc = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(testLoader, disable=True):\n",
    "#                 data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "#                 out = model(data)\n",
    "#                 loss = criterion(out, labels)\n",
    "#                 preds = out.argmax(dim=1)\n",
    "#                 test_loss += loss.item()\n",
    "#                 test_acc += (preds == labels).sum().item()\n",
    "\n",
    "#         test_loss /= len(testLoader)\n",
    "#         test_acc /= len(testLoader.dataset)\n",
    "#         test_accs.append(test_acc)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "#         print(\n",
    "#             f\"epoch {epoch} | train loss {train_loss:.3f} train acc {train_acc:.2f} | test loss {test_loss:.3f} test acc {test_acc:.2f}\"\n",
    "#         )\n",
    "#     return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, trainLoader, testLoader, criterion, optimizer, n_epochs, device, patience=5\n",
    "):\n",
    "    train_losses, train_accs, test_losses, test_accs = [], [], [], []\n",
    "\n",
    "    # early_stopper = EarlyStopping(patience=patience, min_delta=0.001)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # -------------------------\n",
    "        # ðŸ”¹ TRAINING PHASE\n",
    "        # -------------------------\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        progress = tqdm(trainLoader, desc=f\"Epoch {epoch}/{n_epochs}\", leave=False)\n",
    "\n",
    "        for batch in progress:\n",
    "            data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            out = model(data)\n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # update tqdm bar\n",
    "            progress.set_postfix(\n",
    "                {\n",
    "                    \"train_loss\": f\"{running_loss / (total / labels.size(0)):.4f}\",\n",
    "                    \"train_acc\": f\"{correct / total:.3f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # compute final training stats for the epoch\n",
    "        train_loss = running_loss / len(trainLoader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # -------------------------\n",
    "        # ðŸ”¹ VALIDATION PHASE\n",
    "        # -------------------------\n",
    "        model.eval()\n",
    "        test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in testLoader:\n",
    "                data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                out = model(data)\n",
    "                loss = criterion(out, labels)\n",
    "\n",
    "                preds = out.argmax(dim=1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= len(testLoader)\n",
    "        test_acc = test_correct / test_total\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        # -------------------------\n",
    "        # ðŸ”¹ SUMMARY PRINT\n",
    "        # -------------------------\n",
    "        print(\n",
    "            f\"âœ… Epoch {epoch}/{n_epochs} completed | \"\n",
    "            f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | \"\n",
    "            f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # ðŸ”¹ EARLY STOPPING CHECK\n",
    "        # -------------------------\n",
    "        # early_stopper(test_loss, model)\n",
    "\n",
    "        # if early_stopper.early_stop:\n",
    "        #     print(f\"ðŸ›‘ Early stopping triggered at epoch {epoch}!\")\n",
    "        #     break\n",
    "        # early_stopper.load_best_weights(model)\n",
    "\n",
    "    return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ad96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1/6 completed | Train Loss: 2.4219 Acc: 0.446 | Test Loss: 1.8066 Acc: 0.570\n",
      "Restored best model with loss: 1.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 2/6 completed | Train Loss: 1.9377 Acc: 0.540 | Test Loss: 1.6321 Acc: 0.606\n",
      "Restored best model with loss: 1.6321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 3/6 completed | Train Loss: 1.8355 Acc: 0.561 | Test Loss: 1.5722 Acc: 0.619\n",
      "Restored best model with loss: 1.5722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 4/6 completed | Train Loss: 1.7771 Acc: 0.574 | Test Loss: 1.5308 Acc: 0.628\n",
      "Restored best model with loss: 1.5308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 5/6 completed | Train Loss: 1.7376 Acc: 0.583 | Test Loss: 1.4909 Acc: 0.637\n",
      "Restored best model with loss: 1.4909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 6/6 completed | Train Loss: 1.7082 Acc: 0.589 | Test Loss: 1.4818 Acc: 0.639\n",
      "Restored best model with loss: 1.4818\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_losses, test_accs = train(\n",
    "    model, trainLoader, testLoader, criterion, optimizer, n_epochs, device, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "969b1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"weights/cnn_3_conv.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
