{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31e4fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/ml-deploy/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import random\n",
    "# from gradio_demo.model.utils import output_conv_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device is: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad190b",
   "metadata": {},
   "source": [
    "Loading the dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a71f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Xenova/quickdraw-small\")\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0,), (1,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_ops(examples):\n",
    "    examples[\"image\"] = [preprocess(image) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "dataset.set_transform(preprocess_ops)\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"test\"],\n",
    "    dataset[\"valid\"],\n",
    ")\n",
    "# train_dataset = train_dataset.shard(num_shards=3, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879241d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fe5e1",
   "metadata": {},
   "source": [
    "#### With data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67112611",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Xenova/quickdraw-small\")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Validation/Test: No random changes, just format\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    # Apply the \"noisy\" transforms to the list of images\n",
    "    examples[\"image\"] = [train_transforms(image) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "def preprocess_eval(examples):\n",
    "    # Apply the \"clean\" transforms\n",
    "    examples[\"image\"] = [eval_transforms(image) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset  = dataset[\"test\"]\n",
    "val_dataset   = dataset[\"valid\"]\n",
    "\n",
    "train_dataset = train_dataset.shard(num_shards=3, index=0)\n",
    "\n",
    "# 6. Apply the transforms to the specific splits\n",
    "train_dataset.set_transform(preprocess_train)\n",
    "test_dataset.set_transform(preprocess_eval)\n",
    "val_dataset.set_transform(preprocess_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12887ec0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e982d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of trainset: 4500000, testset: 250000\n"
     ]
    }
   ],
   "source": [
    "names = train_dataset.features[\"label\"].names\n",
    "n_classes = len(names)\n",
    "print(f\"size of trainset: {len(train_dataset)}, testset: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d013cd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x710e0c196270>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"labels.json\", \"w\") as f:\n",
    "    json.dump(names, f)\n",
    "\n",
    "\n",
    "def id_to_class(idx):\n",
    "    return names[idx]\n",
    "\n",
    "\n",
    "# visualizing an example\n",
    "idx = torch.randint(0, 100, (1,))\n",
    "img, label = test_dataset[idx][\"image\"], test_dataset[idx][\"label\"]\n",
    "img = img[0].squeeze(dim=0)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694f9a2",
   "metadata": {},
   "source": [
    "Model architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743f9359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device is: {device}\")\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_filters,\n",
    "        hidden_dim,\n",
    "        n_layers,\n",
    "        n_classes,\n",
    "        input_shape=(1,28,28),#(1, img.shape[0], img.shape[1]),\n",
    "        dropout_rate=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(n_filters, 2 * n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(2 * n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_filters, 4 * n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(4 * n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "        conv_out_size = self._get_flat_size(dummy_input)\n",
    "        # print(conv_out_size)\n",
    "        self.input_dim = conv_out_size # 1080  # 2940#1500 # 960\n",
    "        self.inp_layer = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.classifier = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.classifier.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=dropout_rate),\n",
    "                )\n",
    "            )\n",
    "        self.out_layer = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def _get_flat_size(self, x):\n",
    "        \"\"\"Helper class to get the flat size of the input after convolutions\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.inp_layer(torch.flatten(x, start_dim=1))\n",
    "        for layer in self.classifier:\n",
    "            x = layer(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed170d8",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835fa3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, trainLoader, testLoader, criterion, optimizer, n_epochs, device):#, patience=5\n",
    "    train_losses, train_accs, test_losses, test_accs = [], [], [], []\n",
    "\n",
    "    # early_stopper = EarlyStopping(patience=patience, min_delta=0.001)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # -------------------------\n",
    "        # üîπ TRAINING PHASE\n",
    "        # -------------------------\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        progress = tqdm(trainLoader, desc=f\"Epoch {epoch}/{n_epochs}\", leave=False)\n",
    "\n",
    "        for batch in progress:\n",
    "            data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            out = model(data)\n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # update tqdm bar\n",
    "            progress.set_postfix(\n",
    "                {\n",
    "                    \"train_loss\": f\"{running_loss / (total / labels.size(0)):.4f}\",\n",
    "                    \"train_acc\": f\"{correct / total:.3f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # compute final training stats for the epoch\n",
    "        train_loss = running_loss / len(trainLoader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # -------------------------\n",
    "        # üîπ VALIDATION PHASE\n",
    "        # -------------------------\n",
    "        model.eval()\n",
    "        test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in testLoader:\n",
    "                data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                out = model(data)\n",
    "                loss = criterion(out, labels)\n",
    "\n",
    "                preds = out.argmax(dim=1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= len(testLoader)\n",
    "        test_acc = test_correct / test_total\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        # -------------------------\n",
    "        # üîπ SUMMARY PRINT\n",
    "        # -------------------------\n",
    "        print(\n",
    "            f\"‚úÖ Epoch {epoch}/{n_epochs} completed | \"\n",
    "            f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | \"\n",
    "            f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # üîπ EARLY STOPPING CHECK\n",
    "        # -------------------------\n",
    "        # early_stopper(test_loss, model)\n",
    "\n",
    "        # if early_stopper.early_stop:\n",
    "        #     print(f\"üõë Early stopping triggered at epoch {epoch}!\")\n",
    "        #     break\n",
    "        # early_stopper.load_best_weights(model)\n",
    "\n",
    "    return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c14feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, trainLoader, testLoader, criterion, optimizer, n_epochs, device):\n",
    "#     train_losses = []\n",
    "#     train_accs = []\n",
    "#     test_losses = []\n",
    "#     test_accs = []\n",
    "#     for epoch in range(1, n_epochs + 1):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         train_acc = 0\n",
    "#         for batch in tqdm(trainLoader):\n",
    "#             data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "#             out = model(data)\n",
    "#             preds = out.argmax(dim=1)\n",
    "#             loss = criterion(out, labels)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "#             train_acc += (preds == labels).sum().item()\n",
    "\n",
    "#         train_loss /= len(trainLoader)\n",
    "#         train_acc /= len(trainLoader.dataset)\n",
    "#         train_accs.append(train_acc)\n",
    "#         train_losses.append(train_loss)\n",
    "\n",
    "#         model.eval()\n",
    "#         test_loss = 0\n",
    "#         test_acc = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(testLoader, disable=True):\n",
    "#                 data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "#                 out = model(data)\n",
    "#                 loss = criterion(out, labels)\n",
    "#                 preds = out.argmax(dim=1)\n",
    "#                 test_loss += loss.item()\n",
    "#                 test_acc += (preds == labels).sum().item()\n",
    "\n",
    "#         test_loss /= len(testLoader)\n",
    "#         test_acc /= len(testLoader.dataset)\n",
    "#         test_accs.append(test_acc)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "#         print(\n",
    "#             f\"epoch {epoch} | train loss {train_loss:.3f} train acc {train_acc:.2f} | test loss {test_loss:.3f} test acc {test_acc:.2f}\"\n",
    "#         )\n",
    "#     return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e090dce",
   "metadata": {},
   "source": [
    "---\n",
    "#### Training parameters - one single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params 2255065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/ml-deploy/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 120 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-3\n",
    "dropout = 0 # 0.2\n",
    "batch_size = 512\n",
    "n_epochs = 6\n",
    "num_workers = 120\n",
    "hidden_dim = 512\n",
    "n_filters = 64 # 30\n",
    "n_layers = 2 # 2\n",
    "\n",
    "params = {\"n_filters\": n_filters, \"hidden_dim\": hidden_dim, \"n_layers\": n_layers, \"n_classes\": n_classes, \"dropout_rate\": dropout}\n",
    "model = CNN(**params).to(device)\n",
    "n_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of params {n_params}\")\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainLoader, testLoader = (\n",
    "    DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers),\n",
    "    DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad7d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < (self.best_loss - self.min_delta):\n",
    "            # Loss improved! Save the model state and reset counter\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Loss didn't improve enough\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        \"\"\"Restores the model weights from the epoch with the best loss\"\"\"\n",
    "        if self.best_model_state:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "            print(f\"Restored best model with loss: {self.best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ad96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1/6 completed | Train Loss: 1.4956 Acc: 0.636 | Test Loss: 1.3188 Acc: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2/6 completed | Train Loss: 1.2124 Acc: 0.698 | Test Loss: 1.2027 Acc: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3/6 completed | Train Loss: 1.1345 Acc: 0.715 | Test Loss: 1.1688 Acc: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4/6 completed | Train Loss: 1.0877 Acc: 0.726 | Test Loss: 1.1558 Acc: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5/6 completed | Train Loss: 1.0544 Acc: 0.734 | Test Loss: 1.1453 Acc: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6/6 completed | Train Loss: 1.0284 Acc: 0.740 | Test Loss: 1.1400 Acc: 0.716\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_losses, test_accs = train(\n",
    "    model, trainLoader, testLoader, criterion, optimizer, n_epochs, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbbbf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"weights/cnn_70_accuracy.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cee513",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Search - multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_workers = 40\n",
    "\n",
    "trainLoader, testLoader = (\n",
    "    DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers),\n",
    "    DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98235968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_search(search_space,key):\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_params = {}\n",
    "    values_to_test = search_space[key]\n",
    "    trials = len(values_to_test)\n",
    "    \n",
    "    print(f\"Starting optimization for key: '{key}' with {trials} values: {values_to_test}\")\n",
    "\n",
    "    for i, val in enumerate(values_to_test):\n",
    "        # 1. Sample random parameters\n",
    "        params = {}\n",
    "        for k, v in search_space.items():\n",
    "            if k == key:\n",
    "                # Force the specific value for the key we are testing\n",
    "                params[k] = val\n",
    "            elif isinstance(v, list):\n",
    "                # If other params are lists, pick one randomly (or you could pick the first)\n",
    "                params[k] = random.choice(v)\n",
    "            else:\n",
    "                # If it's a scalar (single number), just use it\n",
    "                params[k] = v\n",
    "\n",
    "        print(f\"\\n--- Trial {i+1}/{trials} ---\")\n",
    "        print(f\"Testing params: {params}\")\n",
    "                \n",
    "        # 3. Initialize Model with new hidden_dim and dropout\n",
    "        model = CNN(\n",
    "            n_filters=30, \n",
    "            hidden_dim=params['hidden_dim'], \n",
    "            n_layers=2, \n",
    "            n_classes=n_classes,\n",
    "            dropout_rate=params['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        n_params = sum([p.numel() for p in model.parameters()])\n",
    "        print(f\"Number of params {n_params}\")\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params[\"weight_decay\"])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # 4. Train for fewer epochs (e.g., 5-10) just to check potential\n",
    "        # We don't need to run full 50 epochs to know if parameters are bad\n",
    "        _, _, _, test_accs = train(\n",
    "            model, trainLoader, testLoader, criterion, optimizer, \n",
    "            n_epochs=6, # Short run for speed\n",
    "            device=device,\n",
    "        )\n",
    "        print(f\"Test accuracy for this model: {test_accs[-1]}\")\n",
    "        \n",
    "        # 5. Compare results\n",
    "        final_acc = test_accs[-1]\n",
    "        if final_acc > best_acc:\n",
    "            best_acc = final_acc\n",
    "            best_params = params\n",
    "            print(f\"üöÄ New Best Accuracy: {best_acc:.3f}\")\n",
    "            \n",
    "    print(f\"\\nüèÜ Optimization Finished! Best Params: {best_params} with Acc: {best_acc}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb1ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "        \"lr\": [0.005,0.01],\n",
    "        \"dropout\": 0,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"weight_decay\":1e-3,\n",
    "    }\n",
    "best_hyperparams = multiple_search(search_space, key='lr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
