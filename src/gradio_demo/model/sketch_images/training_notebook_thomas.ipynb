{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31e4fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/ml-deploy/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from gradio_demo.model.utils import output_conv_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device is: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad190b",
   "metadata": {},
   "source": [
    "Loading the dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67112611",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Xenova/quickdraw-small\")\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0,), (1,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_ops(examples):\n",
    "    examples[\"image\"] = [preprocess(image) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "dataset.set_transform(preprocess_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e982d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of trainset: 1500000, testset: 250000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, val_dataset = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"test\"],\n",
    "    dataset[\"valid\"],\n",
    ")\n",
    "train_dataset = train_dataset.shard(num_shards=3, index=0)\n",
    "names = train_dataset.features[\"label\"].names\n",
    "n_classes = len(names)\n",
    "print(f\"size of trainset: {len(train_dataset)}, testset: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d013cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"labels.json\", \"w\") as f:\n",
    "    json.dump(names, f)\n",
    "\n",
    "\n",
    "def id_to_class(idx):\n",
    "    return names[idx]\n",
    "\n",
    "\n",
    "# visualizing an example\n",
    "idx = torch.randint(0, 100, (1,))\n",
    "img, label = test_dataset[idx][\"image\"], test_dataset[idx][\"label\"]\n",
    "img = img[0].squeeze(dim=0)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694f9a2",
   "metadata": {},
   "source": [
    "Model architechture\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f9359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device is: {device}\")\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_filters,\n",
    "        hidden_dim,\n",
    "        n_layers,\n",
    "        n_classes,\n",
    "        input_shape=(1, img.shape[0], img.shape[1]),\n",
    "        dropout_rate=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(n_filters, 2 * n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(2 * n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_filters, 4 * n_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(4 * n_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(1, n_filters, conv_kernel_size)\n",
    "        # self.relu1 = nn.ReLU()\n",
    "        # self.maxpool1 = nn.MaxPool2d(maxpool_kernel_size)\n",
    "        # self.conv2 = nn.Conv2d(n_filters, 2*n_filters, conv_kernel_size)\n",
    "        # self.relu2 = nn.ReLU()\n",
    "        # self.maxpool2 = nn.MaxPool2d(maxpool_kernel_size)\n",
    "        # self.input_dim = output_conv_size(img.shape[0], img.shape[1],conv_kernel_size=conv_kernel_size,maxpool_kernel_size=maxpool_kernel_size,n_filters=n_filters)#960\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "        conv_out_size = self._get_flat_size(dummy_input)\n",
    "        print(conv_out_size)\n",
    "        self.input_dim = 1080  # 2940#1500 # 960\n",
    "        self.inp_layer = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.classifier = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.classifier.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=dropout_rate),\n",
    "                )\n",
    "            )\n",
    "        self.out_layer = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def _get_flat_size(self, x):\n",
    "        \"\"\"Helper class to get the flat size of the input after convolutions\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.inp_layer(torch.flatten(x, start_dim=1))\n",
    "        for layer in self.classifier:\n",
    "            x = layer(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb51eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params 579909\n"
     ]
    }
   ],
   "source": [
    "params = {\"n_filters\": 30, \"hidden_dim\": 256, \"n_layers\": 2, \"n_classes\": n_classes}\n",
    "model = CNN(**params).to(device)\n",
    "n_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of params {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/ml-deploy/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 40 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 512\n",
    "n_epochs = 6\n",
    "num_workers = 40\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainLoader, testLoader = (\n",
    "    DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers),\n",
    "    DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad7d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < (self.best_loss - self.min_delta):\n",
    "            # Loss improved! Save the model state and reset counter\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Loss didn't improve enough\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        \"\"\"Restores the model weights from the epoch with the best loss\"\"\"\n",
    "        if self.best_model_state:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "            print(f\"Restored best model with loss: {self.best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ee08a",
   "metadata": {},
   "source": [
    "Training loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e53e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, trainLoader, testLoader, criterion, optimizer, n_epochs, device):\n",
    "#     train_losses = []\n",
    "#     train_accs = []\n",
    "#     test_losses = []\n",
    "#     test_accs = []\n",
    "#     for epoch in range(1, n_epochs + 1):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         train_acc = 0\n",
    "#         for batch in tqdm(trainLoader):\n",
    "#             data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "#             out = model(data)\n",
    "#             preds = out.argmax(dim=1)\n",
    "#             loss = criterion(out, labels)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "#             train_acc += (preds == labels).sum().item()\n",
    "\n",
    "#         train_loss /= len(trainLoader)\n",
    "#         train_acc /= len(trainLoader.dataset)\n",
    "#         train_accs.append(train_acc)\n",
    "#         train_losses.append(train_loss)\n",
    "\n",
    "#         model.eval()\n",
    "#         test_loss = 0\n",
    "#         test_acc = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(testLoader, disable=True):\n",
    "#                 data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "#                 out = model(data)\n",
    "#                 loss = criterion(out, labels)\n",
    "#                 preds = out.argmax(dim=1)\n",
    "#                 test_loss += loss.item()\n",
    "#                 test_acc += (preds == labels).sum().item()\n",
    "\n",
    "#         test_loss /= len(testLoader)\n",
    "#         test_acc /= len(testLoader.dataset)\n",
    "#         test_accs.append(test_acc)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "#         print(\n",
    "#             f\"epoch {epoch} | train loss {train_loss:.3f} train acc {train_acc:.2f} | test loss {test_loss:.3f} test acc {test_acc:.2f}\"\n",
    "#         )\n",
    "#     return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, trainLoader, testLoader, criterion, optimizer, n_epochs, device, patience=5\n",
    "):\n",
    "    train_losses, train_accs, test_losses, test_accs = [], [], [], []\n",
    "\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=0.001)\n",
    "\n",
    "    # epoch_bar = tqdm(range(1, n_epochs + 1), desc=\"Overall Training\", leave=True)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # -------------------------\n",
    "        # ðŸ”¹ TRAINING PHASE\n",
    "        # -------------------------\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        progress = tqdm(trainLoader, desc=f\"Epoch {epoch}/{n_epochs}\", leave=False)\n",
    "\n",
    "        for batch in progress:\n",
    "            data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            out = model(data)\n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # update tqdm bar\n",
    "            progress.set_postfix(\n",
    "                {\n",
    "                    \"train_loss\": f\"{running_loss / (total / labels.size(0)):.4f}\",\n",
    "                    \"train_acc\": f\"{correct / total:.3f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # compute final training stats for the epoch\n",
    "        train_loss = running_loss / len(trainLoader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # -------------------------\n",
    "        # ðŸ”¹ VALIDATION PHASE\n",
    "        # -------------------------\n",
    "        model.eval()\n",
    "        test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in testLoader:\n",
    "                data, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                out = model(data)\n",
    "                loss = criterion(out, labels)\n",
    "\n",
    "                preds = out.argmax(dim=1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= len(testLoader)\n",
    "        test_acc = test_correct / test_total\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        # -------------------------\n",
    "        # ðŸ”¹ SUMMARY PRINT\n",
    "        # -------------------------\n",
    "        print(\n",
    "            f\"âœ… Epoch {epoch}/{n_epochs} completed | \"\n",
    "            f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | \"\n",
    "            f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # ðŸ”¹ EARLY STOPPING CHECK\n",
    "        # -------------------------\n",
    "        early_stopper(test_loss, model)\n",
    "\n",
    "        if early_stopper.early_stop:\n",
    "            print(f\"ðŸ›‘ Early stopping triggered at epoch {epoch}!\")\n",
    "            break\n",
    "        early_stopper.load_best_weights(model)\n",
    "\n",
    "    return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1/6 completed | Train Loss: 2.8716 Acc: 0.356 | Test Loss: 1.9950 Acc: 0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 2/6 completed | Train Loss: 2.3304 Acc: 0.458 | Test Loss: 1.8038 Acc: 0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 3/6 completed | Train Loss: 2.2149 Acc: 0.482 | Test Loss: 1.7546 Acc: 0.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 4/6 completed | Train Loss: 2.1482 Acc: 0.497 | Test Loss: 1.7046 Acc: 0.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 5/6 completed | Train Loss: 2.1033 Acc: 0.507 | Test Loss: 1.6695 Acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 6/6 completed | Train Loss: 2.0703 Acc: 0.514 | Test Loss: 1.6425 Acc: 0.607\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_losses, test_accs = train(\n",
    "    model, trainLoader, testLoader, criterion, optimizer, n_epochs, device, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"weights/cnn_3_conv.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
