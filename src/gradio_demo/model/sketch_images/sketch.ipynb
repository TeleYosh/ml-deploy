{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ee81d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yosh/gradio-demo/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Xenova/quickdraw-small\")\n",
    "\n",
    "# could be used for data-augmentation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0,), (1,))  \n",
    "])\n",
    "\n",
    "def preprocess_ops(examples):\n",
    "    examples['image'] = [preprocess(image) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "dataset.set_transform(preprocess_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9c612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 4500000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 250000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 250000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size 450000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, val_dataset = dataset['train'], dataset['test'], dataset['valid']\n",
    "train_dataset = train_dataset.shard(num_shards=10, index=0)\n",
    "names = train_dataset.features['label'].names\n",
    "n_classes = len(names)\n",
    "\n",
    "print(f'trainset size {len(train_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('labels.json', 'w') as f:\n",
    "    json.dump(names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c21279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_class(idx):\n",
    "    return names[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88205332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label spider\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff6a386dd30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "idx = torch.randint(0, 100, (1,))\n",
    "img, label = test_dataset[idx]['image'], test_dataset[idx]['label']\n",
    "img = img[0].squeeze(dim=0)\n",
    "print(f'label {id_to_class(label[0])}')\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0e859",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params 197385\n"
     ]
    }
   ],
   "source": [
    "from cnn import CNN # type: ignore\n",
    "\n",
    "params = {\n",
    "    'n_filters': 30,\n",
    "    'hidden_dim': 100,\n",
    "    'n_layers': 2,\n",
    "    'n_classes': n_classes\n",
    "}\n",
    "model = CNN(**params).to(device)\n",
    "n_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f'Number of params {n_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f92edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001\n",
    "batch_size = 128*4\n",
    "n_epochs = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainLoader, testLoader = DataLoader(train_dataset, batch_size=batch_size), DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37308dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainLoader, testLoader, criterion, optimizer, n_epochs):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for batch in tqdm(trainLoader):\n",
    "            data, labels = batch['image'], batch['label']\n",
    "            out = model(data)\n",
    "            preds = out.argmax(dim=1)\n",
    "            loss = criterion(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (preds == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(trainLoader)\n",
    "        train_acc /= len(trainLoader.dataset)\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(testLoader, disable=True):\n",
    "                data, labels = batch['image'], batch['label']\n",
    "                out = model(data)\n",
    "                loss = criterion(out, labels)\n",
    "                preds = out.argmax(dim=1)\n",
    "                test_loss += loss.item()\n",
    "                test_acc += (preds == labels).sum().item()\n",
    "\n",
    "        test_loss /= len(testLoader)\n",
    "        test_acc /= len(testLoader.dataset)\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f'epoch {epoch} | train loss {train_loss:.3f} train acc {train_acc:.2f} | test loss {test_loss:.3f} test acc {test_acc:.2f}')\n",
    "    return train_losses, train_accs, test_losses, test_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b090d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1758/1758 [03:07<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | train loss 3.754 train acc 0.21 | test loss 2.756 test acc 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1758/1758 [02:58<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 | train loss 2.980 train acc 0.33 | test loss 2.439 test acc 0.44\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_losses, test_accs = train(model, trainLoader, testLoader, criterion, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d149ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if max(test_accs) > 0.8:\n",
    "    torch.save(model.state_dict(), 'weights/cnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db9031",
   "metadata": {},
   "source": [
    "Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a8fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 153/977 [00:06<00:37, 22.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio_demo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_validation_metrics\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# def get_validation_metrics(model, val_dataset, batch_size, criterion, device):\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#     valLoader = DataLoader(val_dataset, batch_size=batch_size)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     all_preds, all_labels = [], []\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#     return metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m val_metrics = \u001b[43mget_validation_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradio-demo/src/gradio_demo/model/utils.py:95\u001b[39m, in \u001b[36mget_validation_metrics\u001b[39m\u001b[34m(model, val_dataset, batch_size, criterion, device)\u001b[39m\n\u001b[32m     92\u001b[39m data: Tensor = batch[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     93\u001b[39m labels: Tensor = batch[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m outputs: Tensor = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m loss: Tensor = criterion(outputs, labels)\n\u001b[32m     98\u001b[39m preds = outputs.argmax(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradio-demo/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradio-demo/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradio-demo/src/gradio_demo/model/sketch_images/cnn.py:27\u001b[39m, in \u001b[36mCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.maxpool1(\u001b[38;5;28mself\u001b[39m.relu1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     28\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.maxpool2(\u001b[38;5;28mself\u001b[39m.relu2(\u001b[38;5;28mself\u001b[39m.conv2(x)))\n\u001b[32m     29\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.inp_layer(torch.flatten(x, start_dim=\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradio-demo/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradio-demo/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradio-demo/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradio-demo/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from gradio_demo.model.utils import get_validation_metrics\n",
    "\n",
    "# def get_validation_metrics(model, val_dataset, batch_size, criterion, device):\n",
    "#     valLoader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "#     all_preds, all_labels = [], []\n",
    "#     val_loss, val_acc = 0, 0\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(valLoader):\n",
    "#             data, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "#             out = model(data)\n",
    "#             loss = criterion(out, labels)\n",
    "#             preds = out.argmax(dim=1)\n",
    "#             val_loss += loss.item()\n",
    "#             val_acc += (preds == labels).sum().item()\n",
    "#             all_preds.extend(preds.detach().cpu().numpy())\n",
    "#             all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "#         val_loss /= len(valLoader)\n",
    "#         val_acc /= len(valLoader.dataset)\n",
    "\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "#         all_labels, all_preds, average='weighted'  # or 'weighted' depending on your preference\n",
    "#     )\n",
    "#     metrics = {\n",
    "#         'loss': val_loss,\n",
    "#         'accuracy': val_acc,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'f1_score': f1\n",
    "#     }\n",
    "#     return metrics\n",
    "\n",
    "val_metrics = get_validation_metrics(model, val_dataset, batch_size=256, criterion=criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb74a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 2.439\n",
      "validation_accuracy: 0.442\n",
      "precision: 0.459\n",
      "recall: 0.442\n",
      "f1_score: 0.423\n"
     ]
    }
   ],
   "source": [
    "for k, v in val_metrics.items():\n",
    "    print(f'{k}: {v:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bdd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
